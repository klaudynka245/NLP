{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'id': 'bielik', 'full_name': 'speakleash/Bielik-11B-v2.2-Instruct', 'name': 'speakleash/Bielik-11B-v2.2-Instruct'}, {'id': 'cohere', 'full_name': 'CohereForAI/c4ai-command-r-plus', 'name': 'CohereForAI/c4ai-command-r-plus'}, {'id': 'mixtral-8x22B', 'full_name': 'mistralai/Mixtral-8x22B-Instruct-v0.1', 'name': 'mistralai/Mixtral-8x22B-Instruct-v0.1'}, {'id': 'llama3.1-8b', 'full_name': 'meta-llama/Llama-3.1-8B-Instruct', 'name': 'meta-llama/Llama-3.1-8B-Instruct'}, {'id': 'llama', 'full_name': 'meta-llama/Llama-3.1-8B-Instruct', 'name': 'meta-llama/Llama-3.1-8B-Instruct'}, {'id': 'llama-guard', 'full_name': 'meta-llama/Llama-Guard-3-8B', 'name': 'meta-llama/Llama-Guard-3-8B'}, {'id': 'llama3.1', 'full_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'name': 'meta-llama/Meta-Llama-3.1-70B-Instruct'}, {'id': 'openchat', 'full_name': 'openchat/openchat-3.5-1210', 'name': 'openchat/openchat-3.5-1210'}, {'id': 'pllum-shparag-llama3-8B', 'full_name': 'pllum/rag_70k_nemotron_finetune_pol_llama8b', 'name': 'pllum/rag_70k_nemotron_finetune_pol_llama8b'}, {'id': 'pllum-rag-shparag', 'full_name': 'pllum/mixtral_8x7B_pl_max1k_with_2335_dialog_and_external_data_v3_max_1k_mvpv2_v0.1', 'name': 'pllum/mixtral_8x7B_pl_max1k_with_2335_dialog_and_external_data_v3_max_1k_mvpv2_v0.1'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"CLARIN_API_KEY\")\n",
    "\n",
    "MODELS_ENDPOINT = \"https://services.clarin-pl.eu/api/v1/oapi/models\"\n",
    "COMPLETIONS_ENDPOINT = \"https://services.clarin-pl.eu/api/v1/oapi/chat/completions\"\n",
    "\n",
    "def get_models():\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {API_KEY}',  \n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.get(MODELS_ENDPOINT, headers=headers)  \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "\n",
    "models = get_models()\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(dataset, model):\n",
    "    reviews = dataset[\"train\"][\"text\"] \n",
    "    embeddings = model.encode(reviews, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  \n",
    "    index.add(embeddings)  \n",
    "    return index, reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    result_reviews = [reviews[idx] for idx in indices[0]]\n",
    "    return distances[0], result_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarin_chat_completion(model_id, prompt, max_tokens=200):\n",
    "    url = f\"{CLARIN_BASE_URL}/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"CLARIN API Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query_with_rag(user_query, model_id=\"bielik\", k=5):\n",
    "    distances, result_reviews = search_faiss(user_query, k)\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Recenzja {i+1}: {review}\" for i, review in enumerate(result_reviews)])\n",
    "    if not context.strip():\n",
    "        context = \"Brak dodatkowego kontekstu.\"\n",
    "\n",
    "    prompt = f\"Zapytanie: {user_query}\\n\\nKontekst:\\n{context}\\n\\nOdpowiedz w kontekście powyższych informacji.\"\n",
    "    response = clarin_chat_completion(model_id, prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index, reviews = build_faiss_index(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'index/imdb_faiss.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź: Wybór najlepszego filmu do oglądania z przyjaciółmi zależy od kilku czynników, takich jak gust grupy, okazja oraz tematyka filmu. Ze swojej perspektywy mogę polecić kilka tytułów, które nadają się znakomicie na seans w towarzystwie znajomych.\n",
      "\n",
      "Jeśli szukasz lekkiego i pełnego humoru filmu, świetną opcją będą komedie romantyczne, takie jak \"Notting Hill\" czy \"Dziewczyna z sąsiedztwa\". Oba te filmy nie tylko zapewnią sporo śmiechu, ale także stworzą miłą atmosferę do rozmowy po projekcji.\n",
      "\n",
      "Dla fanów animacji polecam \"Toy Story\", serię filmów o Toy Story jest niezwykle uroczym i uniwersalnym wyborem, który spodoba się widzom w każdym wieku. Filmy te mają znakomitą fabułę oraz piękne przesłanie, a ich różnorodność sprawi, że nikt się nie znudzi.\n",
      "\n",
      "Miłośnicy kina akcji mogą cieszyć się przygodami w cyklu \"Mad Max\" - te niezwykle dynamiczne i pełne spektakularnych efektów specjalnych produkcje zapewnią mocne wrażenia i dodadzą energii do spotkania ze znajomymi.\n",
      "\n",
      "A jeśli chcesz wprowadzić intelektualny wymiar do waszej sesji filmowej, spróbujcie poruszyć ważkie kwestie w filmach takich jak \"Siedem dusz\" czy \"Prestige\". Obydwa te filmy oferują głębię psychologiczną i moralne dylematy, co może stanowić doskonały punkt wyjścia do interesujących d\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the best movie to watch with friends?\"\n",
    "response = handle_query_with_rag(user_query)\n",
    "print(\"Odpowiedź:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W oparciu o podane informacje, nie możemy bezpośrednio określić dokładnej opinii ludzi o horrorach, ponieważ zamiast treści recenzji mamy tylko numery referencyjne. Jednakże na ich podstawie możemy sformułować następujące domysły:\\n\\n1. Skoro są to recenzje filmów grozy, prawdopodobnie ludzie mają różnorodne opinie - od negatywnych po entuzjastyczne. \\n\\n2. Numery recenzji sugerują, że opinie byłyby rozłożone na kilka tysięcy widzów (np. dla recenzji 23708 - prawie 24 tysiące). W tak dużej grupie spodziewalibyśmy się również pogłębionej różnorodności preferencji i emocji wobec horrorów.\\n\\n3. Możliwe, że niektóre z tych recenzji dotyczą klasyków gatunku jak \"Psychoza\" czy \"Piła\", inne mogłyby omawiać najnowsze produkcje lub niszowe kino grozy.\\n\\nAby uzyskać szczegółową opinię publiczną na temat horrorów, potrzebne byłyby pełne treści tych recenzji oraz dodatkowe dane ze sprzedaży biletów, liczby wyświetleń czy reakcji w mediach społecznościowych. Jednocześnie pamiętajmy, że opinie osobiste ulegają zmianom i są konstruowane z różnych doświadczeń, uprzedzeń oraz osobistych doświadczeń widzów.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_query_with_rag(\"What do people think about horror movies?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'StandardGpuResources'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Tworzenie FAISS Index\u001b[39;00m\n\u001b[0;32m    126\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m index, reviews \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m, in \u001b[0;36mbuild_faiss_index\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m     58\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexIVFPQ(quantizer, d, nlist, m, nbits)  \u001b[38;5;66;03m# Tworzenie indeksu IVFPQ\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Sprawdzenie, czy FAISS został przeniesiony na GPU\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_faiss_on_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Funkcja do przenoszenia na GPU (opcjonalna)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m index\u001b[38;5;241m.\u001b[39mtrain(embeddings)  \u001b[38;5;66;03m# Trening FAISS na wektorach\u001b[39;00m\n\u001b[0;32m     64\u001b[0m index\u001b[38;5;241m.\u001b[39madd(embeddings)  \u001b[38;5;66;03m# Dodanie wektorów do indeksu\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36minitialize_faiss_on_gpu\u001b[1;34m(index, quantizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_faiss_on_gpu\u001b[39m(index, quantizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 19\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardGpuResources\u001b[49m()\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m quantizer:\n\u001b[0;32m     21\u001b[0m             quantizer \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mindex_cpu_to_gpu(res, \u001b[38;5;241m0\u001b[39m, quantizer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'faiss' has no attribute 'StandardGpuResources'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "\n",
    "# Wczytanie API Key dla CLARIN\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"CLARIN_API_KEY\")\n",
    "\n",
    "# Ustawienia dla CLARIN API\n",
    "CLARIN_BASE_URL = \"https://services.clarin-pl.eu/api/v1/oapi\"\n",
    "\n",
    "# FAISS GPU Check\n",
    "def initialize_faiss_on_gpu(index, quantizer=None):\n",
    "    if torch.cuda.is_available():\n",
    "        res = faiss.GpuResources()\n",
    "        if quantizer:\n",
    "            quantizer = faiss.index_cpu_to_gpu(res, 0, quantizer)\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    return index\n",
    "\n",
    "# Funkcja do pobierania modeli z CLARIN API\n",
    "def get_models():\n",
    "    headers = {'Authorization': f'Bearer {API_KEY}', 'Content-Type': 'application/json'}\n",
    "    response = requests.get(f\"{CLARIN_BASE_URL}/models\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch models: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Pobranie IMDB Dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# FAISS z IndexIVFFlat\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Funkcja do budowy indeksu FAISS z użyciem Product Quantization (IndexIVFPQ)\n",
    "def build_faiss_index(dataset, model):\n",
    "    reviews = dataset[\"train\"][\"text\"]\n",
    "    embeddings = model.encode(reviews, convert_to_numpy=True).astype(\"float32\")\n",
    "    \n",
    "    d = embeddings.shape[1]  # Wymiar danych\n",
    "    quantizer = faiss.IndexFlatL2(d)  # Klasteryzator\n",
    "\n",
    "    # Tworzymy indeks IVFPQ z 100 klastrami, m=8 (liczba części kwantyzacji), nbits=8 (rozmiar każdej części)\n",
    "    nlist = 100  # Liczba klastrów\n",
    "    m = 8  # Liczba części kwantyzacji\n",
    "    nbits = 8  # Liczba bitów przypisanych do każdej części\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbits)  # Tworzenie indeksu IVFPQ\n",
    "\n",
    "    # Sprawdzenie, czy FAISS został przeniesiony na GPU\n",
    "    index = initialize_faiss_on_gpu(index)  # Funkcja do przenoszenia na GPU (opcjonalna)\n",
    "\n",
    "    index.train(embeddings)  # Trening FAISS na wektorach\n",
    "    index.add(embeddings)  # Dodanie wektorów do indeksu\n",
    "\n",
    "    # Zapisz indeks na dysk\n",
    "    faiss.write_index(index, \"index/imdb_faiss_ivfpq.bin\")\n",
    "    print(\"Indeks został zapisany na dysk.\")\n",
    "\n",
    "    return index, reviews\n",
    "\n",
    "\n",
    "# Funkcja wyszukiwania FAISS\n",
    "def search_faiss(index, query, model, reviews, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    result_reviews = [reviews[idx] for idx in indices[0]]\n",
    "    return distances[0], result_reviews\n",
    "\n",
    "# Reranker z Cross-Encoder\n",
    "def rerank_with_cross_encoder(query, contexts):\n",
    "    reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    rerank_scores = reranker.predict([(query, context) for context in contexts])\n",
    "    sorted_indices = sorted(range(len(rerank_scores)), key=lambda i: rerank_scores[i], reverse=True)\n",
    "    return [contexts[i] for i in sorted_indices], rerank_scores\n",
    "\n",
    "# Chat Completion z CLARIN\n",
    "def clarin_chat_completion(model_id, prompt, max_tokens=200):\n",
    "    url = f\"{CLARIN_BASE_URL}/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"CLARIN API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Funkcja obsługi zapytania z RAG\n",
    "def handle_query_with_rag(user_query, model_id=\"bielik\", k=5):\n",
    "    distances, result_reviews = search_faiss(index, user_query, model, reviews, k)\n",
    "\n",
    "    # Reranking\n",
    "    reranked_reviews, rerank_scores = rerank_with_cross_encoder(user_query, result_reviews)\n",
    "\n",
    "    # Przygotowanie kontekstu\n",
    "    context = \"\\n\\n\".join([f\"Recenzja {i+1}: {review}\" for i, review in enumerate(reranked_reviews)])\n",
    "    if not context.strip():\n",
    "        context = \"Brak dodatkowego kontekstu.\"\n",
    "\n",
    "    prompt = f\"Zapytanie: {user_query}\\n\\nKontekst:\\n{context}\\n\\nOdpowiedz w kontekście powyższych informacji.\"\n",
    "    response = clarin_chat_completion(model_id, prompt)\n",
    "    return response, context\n",
    "\n",
    "# Ocena odpowiedzi modelem LLaMA\n",
    "def evaluate_response_with_llama(question, context, answer):\n",
    "    evaluator = pipeline(\"text-classification\", model=\"meta-llama/LLaMA-2-7b-hf\")\n",
    "    prompt = f\"Czy odpowiedź '{answer}' na pytanie '{question}' jest poprawna w kontekście: {context}?\"\n",
    "    evaluation = evaluator(prompt)\n",
    "    return evaluation\n",
    "\n",
    "# Tworzenie FAISS Index\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index, reviews = build_faiss_index(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\kjani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kjani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the best movie to watch with friends?\"\n",
    "response, context = handle_query_with_rag(user_query)\n",
    "\n",
    "# Ocena odpowiedzi\n",
    "evaluation = evaluate_response_with_llama(user_query, context, response)\n",
    "print(\"Odpowiedź:\", response)\n",
    "print(\"Ocena:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
