{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nie uzna gola. Robben był kilka metrów w polu ...</td>\n",
       "      <td>[[0, 8, odwrócenie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER No właśnie o tym jest ten tweet 😄</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER @USER Widać chcą wiecej polskich mord go...</td>\n",
       "      <td>[[23, 38, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Idę spać bo padam na twarz, w końcu w domuuuu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER Tak się poznałam z moim chłopakiem 😂 cza...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>@USER Wszystkiego najlepszego z okazji urodzin...</td>\n",
       "      <td>[[5, 29, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>@USER widzę, że pewne tweety działają jak magn...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>@USER @USER Chociaż futro ma z jenota,\\nTo nie...</td>\n",
       "      <td>[[43, 52, odwrócenie], [55, 67, wzmocnienie], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>@USER Ty aby nie zacząleś ćpać przez wydumane ...</td>\n",
       "      <td>[[13, 25, odwrócenie], [37, 54, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>@USER och, to też mój superman w kuchni. Jak c...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Nie uzna gola. Robben był kilka metrów w polu ...   \n",
       "1              @USER No właśnie o tym jest ten tweet 😄   \n",
       "2    @USER @USER Widać chcą wiecej polskich mord go...   \n",
       "3        Idę spać bo padam na twarz, w końcu w domuuuu   \n",
       "4    @USER Tak się poznałam z moim chłopakiem 😂 cza...   \n",
       "..                                                 ...   \n",
       "795  @USER Wszystkiego najlepszego z okazji urodzin...   \n",
       "796  @USER widzę, że pewne tweety działają jak magn...   \n",
       "797  @USER @USER Chociaż futro ma z jenota,\\nTo nie...   \n",
       "798  @USER Ty aby nie zacząleś ćpać przez wydumane ...   \n",
       "799  @USER och, to też mój superman w kuchni. Jak c...   \n",
       "\n",
       "                                                 label  \n",
       "0                                 [[0, 8, odwrócenie]]  \n",
       "1                                                   []  \n",
       "2                              [[23, 38, wzmocnienie]]  \n",
       "3                                                   []  \n",
       "4                                                   []  \n",
       "..                                                 ...  \n",
       "795                             [[5, 29, wzmocnienie]]  \n",
       "796                                                 []  \n",
       "797  [[43, 52, odwrócenie], [55, 67, wzmocnienie], ...  \n",
       "798      [[13, 25, odwrócenie], [37, 54, wzmocnienie]]  \n",
       "799                                                 []  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../task_1/data/fragments_classification.jsonl\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'wzmocnienie': 1, \n",
    "                 'osłabienie': 2,\n",
    "                 'odwrócenie': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(label_list, mapping):\n",
    "    return [[start, end, mapping[label]] for start, end, label in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: map_labels(x, label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nie uzna gola. Robben był kilka metrów w polu ...</td>\n",
       "      <td>[[0, 8, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER No właśnie o tym jest ten tweet 😄</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER @USER Widać chcą wiecej polskich mord go...</td>\n",
       "      <td>[[23, 38, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Idę spać bo padam na twarz, w końcu w domuuuu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER Tak się poznałam z moim chłopakiem 😂 cza...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>@USER Wszystkiego najlepszego z okazji urodzin...</td>\n",
       "      <td>[[5, 29, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>@USER widzę, że pewne tweety działają jak magn...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>@USER @USER Chociaż futro ma z jenota,\\nTo nie...</td>\n",
       "      <td>[[43, 52, 3], [55, 67, 1], [77, 83, 1], [112, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>@USER Ty aby nie zacząleś ćpać przez wydumane ...</td>\n",
       "      <td>[[13, 25, 3], [37, 54, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>@USER och, to też mój superman w kuchni. Jak c...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Nie uzna gola. Robben był kilka metrów w polu ...   \n",
       "1              @USER No właśnie o tym jest ten tweet 😄   \n",
       "2    @USER @USER Widać chcą wiecej polskich mord go...   \n",
       "3        Idę spać bo padam na twarz, w końcu w domuuuu   \n",
       "4    @USER Tak się poznałam z moim chłopakiem 😂 cza...   \n",
       "..                                                 ...   \n",
       "795  @USER Wszystkiego najlepszego z okazji urodzin...   \n",
       "796  @USER widzę, że pewne tweety działają jak magn...   \n",
       "797  @USER @USER Chociaż futro ma z jenota,\\nTo nie...   \n",
       "798  @USER Ty aby nie zacząleś ćpać przez wydumane ...   \n",
       "799  @USER och, to też mój superman w kuchni. Jak c...   \n",
       "\n",
       "                                                 label  \n",
       "0                                          [[0, 8, 3]]  \n",
       "1                                                   []  \n",
       "2                                        [[23, 38, 1]]  \n",
       "3                                                   []  \n",
       "4                                                   []  \n",
       "..                                                 ...  \n",
       "795                                       [[5, 29, 1]]  \n",
       "796                                                 []  \n",
       "797  [[43, 52, 3], [55, 67, 1], [77, 83, 1], [112, ...  \n",
       "798                         [[13, 25, 3], [37, 54, 1]]  \n",
       "799                                                 []  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotated_fragments(labels: list):\n",
    "    frag_dict = {}\n",
    "    for label in labels:\n",
    "        frag_dict[(label[0], label[1])] = label[2]\n",
    "    return frag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(13, 25): 3, (37, 54): 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_annotated_fragments(df['label'][798])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "Tokens in annotated fragments: [9152, 2063, 1057, 2480, 2532]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(df['text'][0], return_offsets_mapping=True)\n",
    "annotated_fragments = get_annotated_fragments(df['label'][0])\n",
    "# Check tokens against annotated fragments\n",
    "tokens_in_annotations = []\n",
    "for idx, (start, end) in enumerate(encoding['offset_mapping']):\n",
    "    token_text = encoding['input_ids'][idx]\n",
    "    for fragment_start, fragment_end in annotated_fragments:\n",
    "        # Check if token overlaps with the fragment\n",
    "        if start < fragment_end and end > fragment_start:\n",
    "            tokens_in_annotations.append(token_text)  # decode token id to string\n",
    "            print(annotated_fragments[(fragment_start, fragment_end)])\n",
    "\n",
    "print(\"Tokens in annotated fragments:\", tokens_in_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9152, 2063, 1057, 2480, 2532, 2175, 2721, 1012, 26211, 2368, 2011, 18818, 11382, 26518, 6005, 2860, 1059, 14955, 2226, 10556, 6826, 24335, 1010, 1059, 22571, 22648, 21293, 18818, 17540, 2666, 4241, 4143, 10975, 4371, 4213, 3351, 2079, 2079, 16313, 3211, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (2, 3), (4, 5), (5, 6), (6, 8), (9, 11), (11, 13), (13, 14), (15, 19), (19, 21), (22, 24), (24, 25), (26, 28), (28, 31), (32, 37), (37, 38), (39, 40), (41, 44), (44, 45), (46, 48), (48, 50), (50, 52), (52, 53), (54, 55), (55, 57), (57, 60), (60, 63), (63, 64), (65, 68), (68, 70), (71, 73), (73, 75), (76, 78), (78, 80), (80, 82), (82, 84), (85, 87), (88, 90), (90, 93), (93, 95), (95, 96), (0, 0)]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in annotated fragments: [(9152, 3), (2063, 3), (29250, 3), (16739, 3), (2229, 3), (1059, 1), (25688, 1), (19042, 1), (2063, 1), (3291, 1), (2100, 1)]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(df['text'][798], return_offsets_mapping=True)\n",
    "annotated_fragments = get_annotated_fragments(df['label'][798])\n",
    "# Check tokens against annotated fragments\n",
    "tokens_in_annotations = []\n",
    "for idx, (start, end) in enumerate(encoding['offset_mapping']):\n",
    "    token_text = encoding['input_ids'][idx]\n",
    "    for fragment_start, fragment_end in annotated_fragments:\n",
    "        # Check if token overlaps with the fragment\n",
    "        if start < fragment_end and end > fragment_start:\n",
    "            tokens_in_annotations.append((token_text, annotated_fragments[(fragment_start, fragment_end)]))\n",
    "\n",
    "print(\"Tokens in annotated fragments:\", tokens_in_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    encoding = tokenizer(row['text'], return_offsets_mapping=True)\n",
    "    annotated_fragments = get_annotated_fragments(row['label'])\n",
    "    # Check tokens against annotated fragments\n",
    "    tokens_in_annotations = []\n",
    "    for idx, (start, end) in enumerate(encoding['offset_mapping']):\n",
    "        token_text = encoding['input_ids'][idx]\n",
    "        for fragment_start, fragment_end in annotated_fragments:\n",
    "            # Check if token overlaps with the fragment\n",
    "            if start < fragment_end and end > fragment_start:\n",
    "                tokens_in_annotations.append((token_text, annotated_fragments[(fragment_start, fragment_end)]))\n",
    "    return tokens_in_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_in_annotations_with_labels'] = df.apply(process_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9152, 3),\n",
       " (2063, 3),\n",
       " (29250, 3),\n",
       " (16739, 3),\n",
       " (2229, 3),\n",
       " (1059, 1),\n",
       " (25688, 1),\n",
       " (19042, 1),\n",
       " (2063, 1),\n",
       " (3291, 1),\n",
       " (2100, 1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_in_annotations_with_labels'][798]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenizacja całego tekstu\n",
    "    tokenized_inputs = tokenizer(example[\"text\"], truncation=True, return_offsets_mapping=True)\n",
    "    labels = np.full(len(tokenized_inputs[\"input_ids\"]), -100)  # Inicjalizacja wszystkich etykiet na -100\n",
    "    \n",
    "    # Dopasowanie etykiet do odpowiednich tokenów\n",
    "    for token_id, label in example[\"tokens_in_annotations_with_labels\"]:  # Przechodzi przez listę adnotacji (token_id, label)\n",
    "        # Znajdź pozycję `token_id` w `input_ids`\n",
    "        token_positions = [i for i, input_id in enumerate(tokenized_inputs[\"input_ids\"]) if input_id == token_id]\n",
    "        for pos in token_positions:\n",
    "            labels[pos] = label\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels.tolist()  # Konwertuj etykiety na listę do zapisania w `Dataset`\n",
    "    tokenized_inputs.pop(\"offset_mapping\")  # Usuwamy offset_mapping, nie jest potrzebny w dalszej części\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Przekształcenie kolumny danych na odpowiednie formaty\n",
    "tokenized_dataset = df.apply(tokenize_and_align_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9152, 2063, 1057, 2480, 2532, 2175, 2721, 1012, 26211, 2368, 2011, 18818, 11382, 26518, 6005, 2860, 1059, 14955, 2226, 10556, 6826, 24335, 1010, 1059, 22571, 22648, 21293, 18818, 17540, 2666, 4241, 4143, 10975, 4371, 4213, 3351, 2079, 2079, 16313, 3211, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\kajet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Ustawienia treningowe\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Trener\u001b[39;00m\n\u001b[0;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     24\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m<string>:133\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kajet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1767\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1770\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1771\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kajet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2277\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kajet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\kajet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2150\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2151\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2153\u001b[0m         )\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Wczytanie modelu BERT z nagłówkiem klasyfikacji tokenów\n",
    "num_labels = 3\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Ustawienia treningowe\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trener\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "# Uruchomienie treningu\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(9152, 3), (2063, 3), (1057, 3), (2480, 3), (...\n",
      "1                                                   []\n",
      "2    [(15536, 1), (26005, 1), (3501, 1), (14955, 1)...\n",
      "3                                                   []\n",
      "4                                                   []\n",
      "Name: tokens_in_annotations_with_labels, dtype: object\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sprawdzenie pierwszych kilku wierszy kolumny\n",
    "print(df[\"tokens_in_annotations_with_labels\"].head())\n",
    "print(df[\"tokens_in_annotations_with_labels\"].isnull().sum())  # Liczba wartości NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
