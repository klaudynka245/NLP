{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sdadas/polish-gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text: str, args, num_return_sequences=1) -> str:\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(\n",
    "        tokens.input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=tokens.attention_mask,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        **args\n",
    "    )\n",
    "    output = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "\n",
    "    return output[0] if len(output) == 1 else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER Nic, nic,nic niewa≈ºne, jutro albo w najb...</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER Kibic @USER odpowiada @USER i @USER na k...</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M√≥wi ≈ºe stare rapsy sƒÖ ca≈Çkiem niezle</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER Zaleg≈Ço≈õci by≈Çy, ale wa≈ºne czy by≈Ç...</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Oby nie spierdolil na p√≥≈Çnoc</td>\n",
       "      <td>negatywny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>@USER Noc? To wtedy, gdy jest ciemno? Bo ≈ºadne...</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>wszƒôdzie dobrze, ale w grobie najlepiej</td>\n",
       "      <td>mowa nienawi≈õci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>@USER a ile zagra≈Ç tam minut ?</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>@USER #FinalSix: Mamy to !!! Puchar Polski jes...</td>\n",
       "      <td>pozytywny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>@USER @USER a to ju≈º nie moja wina ≈ºe sprzedal...</td>\n",
       "      <td>neutralny wyd≈∫wiƒôk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4441 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text               label\n",
       "0     @USER Nic, nic,nic niewa≈ºne, jutro albo w najb...  neutralny wyd≈∫wiƒôk\n",
       "1     @USER Kibic @USER odpowiada @USER i @USER na k...  neutralny wyd≈∫wiƒôk\n",
       "2                 M√≥wi ≈ºe stare rapsy sƒÖ ca≈Çkiem niezle  neutralny wyd≈∫wiƒôk\n",
       "3     @USER @USER Zaleg≈Ço≈õci by≈Çy, ale wa≈ºne czy by≈Ç...  neutralny wyd≈∫wiƒôk\n",
       "4              @USER @USER Oby nie spierdolil na p√≥≈Çnoc  negatywny wyd≈∫wiƒôk\n",
       "...                                                 ...                 ...\n",
       "4436  @USER Noc? To wtedy, gdy jest ciemno? Bo ≈ºadne...  neutralny wyd≈∫wiƒôk\n",
       "4437            wszƒôdzie dobrze, ale w grobie najlepiej     mowa nienawi≈õci\n",
       "4438                     @USER a ile zagra≈Ç tam minut ?  neutralny wyd≈∫wiƒôk\n",
       "4439  @USER #FinalSix: Mamy to !!! Puchar Polski jes...  pozytywny wyd≈∫wiƒôk\n",
       "4440  @USER @USER a to ju≈º nie moja wina ≈ºe sprzedal...  neutralny wyd≈∫wiƒôk\n",
       "\n",
       "[4441 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('../task_1/data/full_text_classification.jsonl', lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutralny wyd≈∫wiƒôk    2022\n",
       "negatywny wyd≈∫wiƒôk     920\n",
       "pozytywny wyd≈∫wiƒôk     870\n",
       "mowa nienawi≈õci        629\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nie uzna gola. Robben by≈Ç kilka metr√≥w w polu ...</td>\n",
       "      <td>[[0, 8, odwr√≥cenie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER No w≈Ça≈õnie o tym jest ten tweet üòÑ</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER @USER Widaƒá chcƒÖ wiecej polskich mord go...</td>\n",
       "      <td>[[23, 38, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Idƒô spaƒá bo padam na twarz, w ko≈Ñcu w domuuuu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER Tak siƒô pozna≈Çam z moim ch≈Çopakiem üòÇ cza...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>@USER Wszystkiego najlepszego z okazji urodzin...</td>\n",
       "      <td>[[5, 29, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>@USER widzƒô, ≈ºe pewne tweety dzia≈ÇajƒÖ jak magn...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>@USER @USER Chocia≈º futro ma z jenota,\\nTo nie...</td>\n",
       "      <td>[[43, 52, odwr√≥cenie], [55, 67, wzmocnienie], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>@USER Ty aby nie zaczƒÖle≈õ ƒápaƒá przez wydumane ...</td>\n",
       "      <td>[[13, 25, odwr√≥cenie], [37, 54, wzmocnienie]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>@USER och, to te≈º m√≥j superman w kuchni. Jak c...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Nie uzna gola. Robben by≈Ç kilka metr√≥w w polu ...   \n",
       "1              @USER No w≈Ça≈õnie o tym jest ten tweet üòÑ   \n",
       "2    @USER @USER Widaƒá chcƒÖ wiecej polskich mord go...   \n",
       "3        Idƒô spaƒá bo padam na twarz, w ko≈Ñcu w domuuuu   \n",
       "4    @USER Tak siƒô pozna≈Çam z moim ch≈Çopakiem üòÇ cza...   \n",
       "..                                                 ...   \n",
       "795  @USER Wszystkiego najlepszego z okazji urodzin...   \n",
       "796  @USER widzƒô, ≈ºe pewne tweety dzia≈ÇajƒÖ jak magn...   \n",
       "797  @USER @USER Chocia≈º futro ma z jenota,\\nTo nie...   \n",
       "798  @USER Ty aby nie zaczƒÖle≈õ ƒápaƒá przez wydumane ...   \n",
       "799  @USER och, to te≈º m√≥j superman w kuchni. Jak c...   \n",
       "\n",
       "                                                 label  \n",
       "0                                 [[0, 8, odwr√≥cenie]]  \n",
       "1                                                   []  \n",
       "2                              [[23, 38, wzmocnienie]]  \n",
       "3                                                   []  \n",
       "4                                                   []  \n",
       "..                                                 ...  \n",
       "795                             [[5, 29, wzmocnienie]]  \n",
       "796                                                 []  \n",
       "797  [[43, 52, odwr√≥cenie], [55, 67, wzmocnienie], ...  \n",
       "798      [[13, 25, odwr√≥cenie], [37, 54, wzmocnienie]]  \n",
       "799                                                 []  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_json('../task_1/data/fragments_classification.jsonl', lines=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wzmocnienie': 414, 'odwr√≥cenie': 277, 'os≈Çabienie': 97}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2 = {'wzmocnienie': 0, 'odwr√≥cenie': 0, 'os≈Çabienie': 0}\n",
    "for _, row in df2.iterrows():\n",
    "    for _, _, label in row['label']:\n",
    "        labels2[label] += 1\n",
    "labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_new(df, label, n, args):\n",
    "    texts = df[df['label'] == label]['text'].tolist()\n",
    "    samples = random.choices(texts, k=n)\n",
    "    return [generate(sample[:max(len(sample)//2, 30)], args) for sample in tqdm(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'max_new_tokens': 50, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'no_repeat_ngram_size': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [23:16<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "new_positive = generate_n_new(df, 'pozytywny wyd≈∫wiƒôk', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('new_samples.jsonl', 'w', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_positive:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'pozytywny wyd≈∫wiƒôk'}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [34:05<00:00,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "new_negative = generate_n_new(df, 'negatywny wyd≈∫wiƒôk', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('new_samples.jsonl', 'a', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_negative:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'negatywny wyd≈∫wiƒôk'}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [26:26<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "new_hate_speech = generate_n_new(df, 'mowa nienawi≈õci', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('new_samples.jsonl', 'a', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_hate_speech:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'mowa nienawi≈õci'}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fragments(df, label_name, n, args):\n",
    "    \n",
    "    samples_candidats = []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "\n",
    "        previous_labels = []\n",
    "        for label in labels:\n",
    "            if label[2] == label_name and label[0] > len(text) / 2:\n",
    "                samples_candidats.append((text[:label[0]], previous_labels, label[0], label[2]))\n",
    "                break\n",
    "            previous_labels.append(label)\n",
    "\n",
    "    samples_to_generate = random.choices(samples_candidats, k=n)\n",
    "\n",
    "    new_samples = []\n",
    "    for text, prev_labels, start_idx, label in tqdm(samples_to_generate):\n",
    "        new_sample = generate(text, args)\n",
    "        new_fragment = new_sample[start_idx:]\n",
    "        new_words = new_fragment.split()\n",
    "        if len(new_words) >= 1:\n",
    "            end_idx = start_idx + len(new_words[0])\n",
    "            if len(new_words) >= 2:\n",
    "                end_idx += len(new_words[1]) + 1\n",
    "            new_samples.append({'text': new_sample, 'label': prev_labels + [[start_idx, end_idx, label]]})\n",
    "\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args2 = {'max_new_tokens': 30, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'no_repeat_ngram_size': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '@USER My≈õla≈Çam do dzi≈õ, ≈ºe to cyjanek potasu. A to by≈Ça czysta heroina. - To nie by≈Ço cyjankowe - powiedzia≈Ç. - To by≈Ça czysta heroina,',\n",
       "  'label': [[30, 45, 'os≈Çabienie']]},\n",
       " {'text': '@USER Tyle gotujesz tego ≈ºarcia a .....nie ma co robiƒá z tymi pieniƒôdzmi. No to siƒô zastan√≥w.',\n",
       "  'label': [[6, 19, 'wzmocnienie'], [34, 45, 'os≈Çabienie']]},\n",
       " {'text': '@USER @USER A niby z jakiego powodu? ≈ªeby mog≈Ça sprawowaƒá sw√≥j urzƒÖd? ___ _ - Nie, ≈ºeby mog≈Ça pe≈Çniƒá sw√≥j urzƒÖd. Ale po prostu chce mieƒá co≈õ do powiedzenia w tej sprawie. - A co ona mo≈ºe?',\n",
       "  'label': [[70, 75, 'os≈Çabienie']]},\n",
       " {'text': 'Kiedy≈õ Patryk Jaki m√≥wi≈Ç\\\\\"peda≈Ç√≥w trzeba wyciƒÖƒá\\\\\"teraz na potrzeby kampanii !!\".',\n",
       "  'label': [[34, 47, 'wzmocnienie'], [76, 80, 'os≈Çabienie']]},\n",
       " {'text': 'Ostatnio siƒô zrobi≈Çam polityczna owca. Ale w sumie to nie. Ja to bym chcia≈Ça, ≈ºeby to by≈Ça prawdziwa Polska. ≈ªeby nie by≈Ço podzia≈Ç√≥w. ≈ªeby ludzie byli w sobie szcze',\n",
       "  'label': [[33, 42, 'os≈Çabienie']]}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fragments(df2, 'os≈Çabienie', 5, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "wzmocnienia = generate_fragments(df2, 'wzmocnienie', 50, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [08:53<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "oslabienia = generate_fragments(df2, 'os≈Çabienie', 300, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [03:45<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "odwrocenia = generate_fragments(df2, 'odwr√≥cenie', 150, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = wzmocnienia + oslabienia + odwrocenia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_samples_fragments.jsonl', 'w', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_samples:\n",
    "        jsonl_file.write(json.dumps(entry, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_pos(doc, start_idx, end_idx):\n",
    "    comb = []\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].idx > end_idx:\n",
    "            break\n",
    "        if doc[i].idx + len(doc[i]) > start_idx:\n",
    "            comb.append(doc[i].pos_)\n",
    "    return comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_pos_combinations(df):\n",
    "    profiles = {'wzmocnienie': {}, 'os≈Çabienie': {}, 'odwr√≥cenie': {}}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for start_idx, end_idx, label in labels:\n",
    "            comb = '+'.join(get_label_pos(doc, start_idx, end_idx))\n",
    "            \n",
    "            if comb not in profiles[label]:\n",
    "                profiles[label][comb] = 1\n",
    "            else:\n",
    "                profiles[label][comb] += 1\n",
    "    \n",
    "    return profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wzmocnienie': {'ADJ+ADJ': 3,\n",
       "  'VERB+PUNCT': 12,\n",
       "  'DET+NOUN': 9,\n",
       "  'ADJ+NOUN+PUNCT': 21,\n",
       "  'ADV+ADJ': 9,\n",
       "  'ADJ': 23,\n",
       "  'ADV+PUNCT': 7,\n",
       "  'ADJ+NOUN': 39,\n",
       "  'ADP+NOUN': 7,\n",
       "  'VERB': 19,\n",
       "  'NOUN+VERB': 2,\n",
       "  'PRON': 8,\n",
       "  'VERB+VERB': 4,\n",
       "  'ADV': 21,\n",
       "  'VERB+AUX': 1,\n",
       "  'PART': 6,\n",
       "  'DET': 4,\n",
       "  'NOUN+NOUN': 7,\n",
       "  'ADP+NOUN+PUNCT': 2,\n",
       "  'ADV+ADP': 2,\n",
       "  'VERB+ADP': 2,\n",
       "  'NOUN+ADP': 2,\n",
       "  'NOUN': 44,\n",
       "  'ADV+ADV': 10,\n",
       "  'PART+PROPN': 1,\n",
       "  'PRON+VERB': 4,\n",
       "  'PRON+PART': 1,\n",
       "  'VERB+ADJ': 2,\n",
       "  'NOUN+PUNCT': 11,\n",
       "  'INTJ+PUNCT': 2,\n",
       "  'PRON+NOUN': 3,\n",
       "  'ADV+NOUN': 4,\n",
       "  'ADJ+SCONJ': 1,\n",
       "  'ADV+SPACE+NOUN': 1,\n",
       "  'PART+PART': 3,\n",
       "  'ADV+VERB': 9,\n",
       "  'DET+ADP': 1,\n",
       "  'VERB+ADV': 3,\n",
       "  'PRON+PROPN': 1,\n",
       "  'PRON+ADJ+PUNCT': 2,\n",
       "  'ADJ+PUNCT': 8,\n",
       "  'PROPN+PUNCT': 1,\n",
       "  'SCONJ+VERB+ADV': 1,\n",
       "  'DET+NOUN+PUNCT': 2,\n",
       "  'ADV+ADV+PUNCT': 3,\n",
       "  'INTJ': 6,\n",
       "  'ADP+ADV+PUNCT': 1,\n",
       "  'VERB+NOUN+PUNCT': 3,\n",
       "  'ADV+VERB+PUNCT': 2,\n",
       "  'NOUN+ADJ': 5,\n",
       "  'ADJ+VERB': 1,\n",
       "  'ADP+PRON': 2,\n",
       "  'ADV+PRON': 3,\n",
       "  'PART+ADP': 1,\n",
       "  'DET+ADJ': 4,\n",
       "  'DET+ADJ+PUNCT': 1,\n",
       "  'VERB+NOUN': 5,\n",
       "  'PART+VERB': 1,\n",
       "  'PART+ADJ': 2,\n",
       "  'ADP+ADV': 2,\n",
       "  'VERB+VERB+PUNCT': 1,\n",
       "  'PROPN+VERB': 1,\n",
       "  'DET+VERB': 1,\n",
       "  'VERB+PRON+PUNCT': 2,\n",
       "  'CCONJ+ADV': 1,\n",
       "  'PART+PRON': 1,\n",
       "  'ADV+INTJ': 1,\n",
       "  'NOUN+VERB+PUNCT': 2,\n",
       "  'AUX+NOUN': 1,\n",
       "  'ADJ+ADP+ADJ': 1,\n",
       "  'PROPN': 2,\n",
       "  'PART+VERB+PUNCT': 1,\n",
       "  'INTJ+PART+PUNCT': 1,\n",
       "  'ADV+SYM': 1,\n",
       "  'ADP+DET+NOUN': 1,\n",
       "  'SCONJ': 1,\n",
       "  'PART+ADV+PUNCT': 1,\n",
       "  'ADJ+ADV': 1,\n",
       "  'ADJ+PROPN+PART': 1,\n",
       "  'ADJ+PROPN': 1,\n",
       "  'VERB+PRON': 2,\n",
       "  'ADV+PRON+ADJ': 1,\n",
       "  'ADV+ADJ+PUNCT': 2,\n",
       "  'PART+ADJ+PUNCT': 1,\n",
       "  'NOUN+ADP+NOUN': 2,\n",
       "  'VERB+ADP+NOUN': 1,\n",
       "  'PART+NOUN': 1,\n",
       "  'NOUN+X+X': 1,\n",
       "  'ADJ+ADJ+PUNCT': 1,\n",
       "  'PRON+ADV': 1,\n",
       "  'VERB+ADP+X+X+PROPN+X+CCONJ': 1,\n",
       "  'ADV+ADJ+NOUN+PUNCT': 1,\n",
       "  'PART+AUX': 1,\n",
       "  'ADV+DET+NOUN': 1,\n",
       "  'ADV+ADJ+NOUN': 1,\n",
       "  'ADJ+ADP+NOUN': 1,\n",
       "  'NOUN+NOUN+PUNCT': 1,\n",
       "  'PROPN+NOUN+PUNCT': 1,\n",
       "  'DET+NOUN+ADV+ADJ': 1,\n",
       "  'PRON+ADJ': 1,\n",
       "  'VERB+PROPN': 1},\n",
       " 'os≈Çabienie': {'PART+VERB': 5,\n",
       "  'NOUN+NOUN+PUNCT': 3,\n",
       "  'PART+NOUN+PUNCT': 1,\n",
       "  'ADV+NOUN+PUNCT': 1,\n",
       "  'DET+NOUN': 1,\n",
       "  'NOUN+PUNCT': 5,\n",
       "  'PART': 13,\n",
       "  'VERB+VERB': 3,\n",
       "  'ADJ+NOUN': 1,\n",
       "  'ADJ+PUNCT': 1,\n",
       "  'NOUN': 14,\n",
       "  'PROPN+PUNCT': 2,\n",
       "  'PART+DET': 2,\n",
       "  'PART+NOUN': 3,\n",
       "  'ADV+ADV': 1,\n",
       "  'ADJ+NUM': 1,\n",
       "  'PART+PRON': 3,\n",
       "  'ADV+PRON': 1,\n",
       "  'VERB+PRON+PRON': 1,\n",
       "  'ADV+NOUN': 2,\n",
       "  'ADV+VERB+PUNCT': 2,\n",
       "  'PART+VERB+PUNCT': 1,\n",
       "  'ADV': 2,\n",
       "  'VERB': 1,\n",
       "  'NUM': 1,\n",
       "  'NOUN+ADJ': 1,\n",
       "  'PRON': 1,\n",
       "  'DET+NUM+NOUN': 1,\n",
       "  'NUM+ADJ+NOUN': 1,\n",
       "  'NOUN+AUX': 1,\n",
       "  'VERB+PART': 1,\n",
       "  'ADP+NOUN': 1,\n",
       "  'NOUN+PRON+VERB+PUNCT': 1,\n",
       "  'PART+ADV': 1,\n",
       "  'ADJ+AUX': 1,\n",
       "  'PART+PART+ADV': 1,\n",
       "  'PROPN': 2,\n",
       "  'CCONJ': 1,\n",
       "  'ADP+DET+NOUN': 1,\n",
       "  'VERB+NOUN': 1,\n",
       "  'ADV+ADJ': 1,\n",
       "  'ADV+VERB': 2,\n",
       "  'PART+ADJ': 2,\n",
       "  'PART+VERB+VERB': 1,\n",
       "  'ADV+ADV+PUNCT': 1,\n",
       "  'ADV+NOUN+PUNCT+NOUN+PUNCT': 1,\n",
       "  'SCONJ+AUX': 1,\n",
       "  'DET': 1},\n",
       " 'odwr√≥cenie': {'PART+ADJ': 2,\n",
       "  'PART+VERB': 127,\n",
       "  'ADJ': 10,\n",
       "  'PART+NOUN': 5,\n",
       "  'PART+AUX': 20,\n",
       "  'PART+ADV+PUNCT': 1,\n",
       "  'PART+ADP': 1,\n",
       "  'PART+DET': 2,\n",
       "  'CCONJ+ADP': 2,\n",
       "  'PART+VERB+PUNCT': 26,\n",
       "  'ADP+NOUN': 6,\n",
       "  'CCONJ+PART': 3,\n",
       "  'AUX+CCONJ': 1,\n",
       "  'CCONJ+NOUN': 2,\n",
       "  'CCONJ+ADP+NOUN+PUNCT': 1,\n",
       "  'PART+PUNCT': 6,\n",
       "  'CCONJ+PROPN': 1,\n",
       "  'NOUN+PRON': 1,\n",
       "  'PART+PART+PUNCT': 1,\n",
       "  'PART+VERB+NOUN': 1,\n",
       "  'PRON+PART+VERB+PUNCT': 1,\n",
       "  'NOUN+ADV': 2,\n",
       "  'ADJ+VERB': 1,\n",
       "  'ADP+NOUN+PUNCT': 2,\n",
       "  'PART': 2,\n",
       "  'NOUN': 5,\n",
       "  'PART+PART': 1,\n",
       "  'CCONJ': 1,\n",
       "  'ADV': 5,\n",
       "  'PART+PROPN': 1,\n",
       "  'CCONJ+PART+VERB': 1,\n",
       "  'ADJ+PUNCT': 4,\n",
       "  'ADV+ADV': 1,\n",
       "  'ADV+VERB': 1,\n",
       "  'PART+ADJ+PUNCT': 1,\n",
       "  'PART+AUX+PUNCT': 1,\n",
       "  'PROPN': 2,\n",
       "  'NOUN+NOUN+PUNCT': 1,\n",
       "  'PART+NOUN+PUNCT': 1,\n",
       "  'VERB': 5,\n",
       "  'X': 1,\n",
       "  'CCONJ+DET+NOUN+PUNCT': 1,\n",
       "  'SCONJ+PART': 1,\n",
       "  'PART+VERB+PRON': 1,\n",
       "  'PART+VERB+NOUN+PUNCT': 1,\n",
       "  'PART+CCONJ': 1,\n",
       "  'DET+NOUN+PUNCT': 1,\n",
       "  'PART+VERB+ADJ': 1,\n",
       "  'PART+PRON': 1,\n",
       "  'PRON+ADJ+PUNCT': 1,\n",
       "  'NUM': 1,\n",
       "  'PART+ADV': 2,\n",
       "  'PRON': 1,\n",
       "  'PRON+PRON': 1,\n",
       "  'ADP+NOUN+X': 1,\n",
       "  'SCONJ+VERB': 1,\n",
       "  'PART+NOUN+NOUN': 1,\n",
       "  'CCONJ+PROPN+PUNCT': 1}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles = profile_pos_combinations(df2)\n",
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fragments_with_profile_check(df, label_name, n, args):\n",
    "    \n",
    "    samples_candidats = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "\n",
    "        previous_labels = []\n",
    "        for start_idx, end_idx, label in labels:\n",
    "            if label == label_name and start_idx > len(text) / 2:\n",
    "                samples_candidats.append((text[:start_idx], previous_labels, get_label_pos(nlp(text), start_idx, end_idx), start_idx, end_idx))\n",
    "\n",
    "            previous_labels.append((start_idx, end_idx, label))\n",
    "\n",
    "    samples_to_generate = random.choices(samples_candidats, k=n)\n",
    "\n",
    "    new_samples = []\n",
    "    for text, prev_labels, label_pos, start_idx, end_idx in tqdm(samples_to_generate):\n",
    "        new_n_samples = generate(text, args, 10)\n",
    "        found = False\n",
    "        for new_sample in new_n_samples:\n",
    "            doc = nlp(new_sample)\n",
    "            for i in range(len(doc)):\n",
    "                if start_idx <= doc[i].idx:\n",
    "                    valid = True\n",
    "                    for j, pos in enumerate(label_pos):\n",
    "                        if i+j >= len(doc) or pos != doc[i+j].pos_:\n",
    "                            valid = False\n",
    "                            break\n",
    "                    if valid:\n",
    "                        new_samples.append({'text': new_sample, 'label': prev_labels + [[doc[i].idx, doc[i+j].idx + len(doc[i+j]), label_name]]})\n",
    "                        found = True\n",
    "                        break\n",
    "            if found:\n",
    "                break\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:40<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "wzmocnienia = generate_fragments_with_profile_check(df2, 'wzmocnienie', 50, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:32<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "oslabienia = generate_fragments_with_profile_check(df2, 'os≈Çabienie', 50, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [10:55<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "odwrocenia = generate_fragments_with_profile_check(df2, 'odwr√≥cenie', 150, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = wzmocnienia + oslabienia + odwrocenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('new_samples_fragments.jsonl', 'w', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_samples:\n",
    "        jsonl_file.write(json.dumps(entry, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model2 = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer2.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer2(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences_with_sim_check(df, label, n, args):\n",
    "    texts = df[df['label'] == label]['text'].tolist()\n",
    "    samples = random.choices(texts, k=n)\n",
    "    new_samples = []\n",
    "\n",
    "    for sample in tqdm(samples):\n",
    "        sample_embeddings = get_sentence_embedding(sample)\n",
    "        input = sample[:max(len(sample)//2, 30)]\n",
    "        output = generate(input, args, 10)\n",
    "        best_sample = ''\n",
    "        best_sim = 0\n",
    "        for new_sample in output:\n",
    "            sim = cosine_similarity(sample_embeddings, get_sentence_embedding(new_sample))\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_sample = new_sample\n",
    "        new_samples.append(best_sample)\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [54:37<00:00,  6.56s/it] \n"
     ]
    }
   ],
   "source": [
    "new_positive = generate_sentences_with_sim_check(df, 'pozytywny wyd≈∫wiƒôk', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [55:37<00:00,  6.68s/it]\n"
     ]
    }
   ],
   "source": [
    "new_negative = generate_sentences_with_sim_check(df, 'negatywny wyd≈∫wiƒôk', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [53:26<00:00,  6.41s/it]\n"
     ]
    }
   ],
   "source": [
    "new_hate_speech = generate_sentences_with_sim_check(df, 'mowa nienawi≈õci', 500, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('new_samples.jsonl', 'w', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_positive:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'pozytywny wyd≈∫wiƒôk'}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('new_samples.jsonl', 'a', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_negative:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'negatywny wyd≈∫wiƒôk'}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('new_samples.jsonl', 'a', encoding='utf-8') as jsonl_file:\n",
    "    for entry in new_hate_speech:\n",
    "        jsonl_file.write(json.dumps({'text': entry, 'label': 'mowa nienawi≈õci'}, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
