{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_huggingface(directory: str, dataset: str, task: str) -> None:\n",
    "    os.makedirs(directory, exist_ok = True)\n",
    "    load_dataset(dataset, task, cache_dir = directory, trust_remote_code = True)\n",
    "\n",
    "\n",
    "\n",
    "def load_necessery_files(source: str, destination: str, new_name: str) -> None:\n",
    "    shutil.copy(source, os.path.join(destination, new_name))\n",
    "\n",
    "\n",
    "\n",
    "def clear_dir(dir_to_clear: str) -> None:\n",
    "    for dir in [f for f in os.listdir(dir_to_clear) if os.path.isdir(os.path.join(dir_to_clear, f))]:\n",
    "        shutil.rmtree(dir_to_clear + '/' + dir)\n",
    "\n",
    "\n",
    "\n",
    "def make_sample(file: str, \n",
    "                n: int = 500, \n",
    "                random_state: int = 2137) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, delimiter = r'\\n', header = None)\n",
    "    df = df.replace('@anonymized_account', '@USER', regex=True)\n",
    "    sample = df.sample(n = n, random_state = random_state).reset_index(drop = True)\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def df_to_jsonl(df: pd.DataFrame, output_file: str) -> None:\n",
    "    TEMP = 'temp.json'\n",
    "    df.to_json(TEMP, orient = 'records', lines = True, force_ascii = False)\n",
    "    with open(TEMP, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for line in infile:\n",
    "                data = json.loads(line)\n",
    "                new_format = {\"text\": data[\"0\"], \"label\": \"\"}\n",
    "                json.dump(new_format, outfile, ensure_ascii = False)\n",
    "                outfile.write('\\n')\n",
    "    os.remove(TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 340k/340k [00:00<00:00, 2.52MB/s]\n",
      "Downloading data: 100%|██████████| 70.1k/70.1k [00:00<00:00, 70.2MB/s]\n",
      "Generating train split: 100%|██████████| 10041/10041 [00:00<00:00, 83446.62 examples/s]\n",
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 36890.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Ładowanie zbioru z huggingface\n",
    "DIR = 'data'\n",
    "DATASET = 'poleval/poleval2019_cyberbullying'\n",
    "TASK = 'task01'\n",
    "\n",
    "load_from_huggingface(DIR, DATASET, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przeniesienie pożądanych danych\n",
    "ROOT = DIR + '/downloads/extracted/'\n",
    "DIRS = os.listdir(ROOT)\n",
    "DIRS = [f for f in DIRS if os.path.isdir(os.path.join(ROOT, f))]\n",
    "TRAIN_SRC = ROOT + DIRS[0] + '/training_set_clean_only_text.txt'\n",
    "TEST_SRC = ROOT + DIRS[1] + '/Task6/task 01/test_set_clean_only_text.txt'\n",
    "\n",
    "load_necessery_files(TRAIN_SRC, DIR, new_name = 'train.txt')\n",
    "load_necessery_files(TEST_SRC, DIR, new_name = 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyszczenie zbioru\n",
    "clear_dir(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20meh\\AppData\\Local\\Temp\\ipykernel_10020\\3441968896.py:21: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file, delimiter = r'\\n', header = None)\n"
     ]
    }
   ],
   "source": [
    "# Samplowanie zbioru i zapisanie go w formacie jsonl\n",
    "OUTPUT_NAME = 'data/first_iter.jsonl'\n",
    "FILE = 'data/train.txt'\n",
    "N = 500 # Liczba obserwacji do anotacji w pierwszej iteracji\n",
    "\n",
    "sample = make_sample(file = FILE)\n",
    "df_to_jsonl(sample, OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wikto\\AppData\\Local\\Temp\\ipykernel_49236\\26211251.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data/train.txt', delimiter = r'\\n', header = None)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.txt', delimiter = r'\\n', header = None)\n",
    "df = df.drop_duplicates()\n",
    "df = df.replace('@anonymized_account', '@USER', regex=True)\n",
    "df[\"label\"] = ''\n",
    "df2 = pd.read_json('data/first_iter_fragments.jsonl', lines=True)\n",
    "df = df[~df[0].isin(df2['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n = 100, random_state = 2137).reset_index(drop = True)\n",
    "df_to_jsonl(sample, 'data/second_iter_fragments.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[0].isin(sample[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2331.75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "klaudia = df.sample(n = 2331, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(klaudia[0])]\n",
    "michal = df.sample(n = 2331, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(michal[0])]\n",
    "kajetan = df.sample(n = 2331, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(kajetan[0])]\n",
    "\n",
    "df_to_jsonl(klaudia, 'data/klaudia_all.jsonl')\n",
    "df_to_jsonl(michal, 'data/michal_all.jsonl')\n",
    "df_to_jsonl(kajetan, 'data/kajetan_all.jsonl')\n",
    "df_to_jsonl(df, 'data/wiktor_all.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wikto\\AppData\\Local\\Temp\\ipykernel_22784\\489156739.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data/train.txt', delimiter = r'\\n', header = None)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.txt', delimiter = r'\\n', header = None)\n",
    "df = df.drop_duplicates()\n",
    "df = df.replace('@anonymized_account', '@USER', regex=True)\n",
    "df[\"label\"] = ''\n",
    "df = df[~df[0].str.startswith('RT ')]\n",
    "first = df.sample(n = 100, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(first[0])]\n",
    "second = df.sample(n = 100, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(second[0])]\n",
    "michal = df.sample(n = 200, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(michal[0])]\n",
    "kajetan = df.sample(n = 200, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(kajetan[0])]\n",
    "wiktor = df.sample(n = 200, random_state = 2137).reset_index(drop = True)\n",
    "df = df[~df[0].isin(wiktor[0])]\n",
    "klaudia = df.sample(n = 200, random_state = 2137).reset_index(drop = True)\n",
    "\n",
    "df_to_jsonl(first, 'data/first_iter_fragments.jsonl')\n",
    "df_to_jsonl(second, 'data/second_iter_fragments.jsonl')\n",
    "df_to_jsonl(klaudia, 'data/klaudia_fragments.jsonl')\n",
    "df_to_jsonl(michal, 'data/michal_fragments.jsonl')\n",
    "df_to_jsonl(kajetan, 'data/kajetan_fragments.jsonl')\n",
    "df_to_jsonl(wiktor, 'data/wiktor_fragments.jsonl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
